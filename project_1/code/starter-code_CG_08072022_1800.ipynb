{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 1: Standardized Test Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# Part 1\n",
    "\n",
    "Part 1 requires knowledge of basic Python.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide on your problem statement that will guide your analysis for this project. For guidelines, sample prompts, or inspiration, check out the README.\n",
    "\n",
    "**To-Do:** *Replace this cell with your problem statement.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "- [Background](#Background)\n",
    "- [Data Import & Cleaning](#Data-Import-and-Cleaning)\n",
    "- [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "- [Data Visualization](#Visualize-the-Data)\n",
    "- [Conclusions and Recommendations](#Conclusions-and-Recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SAT and ACT are standardized tests that many colleges and universities in the United States require for their admissions process. This score is used along with other materials such as grade point average (GPA) and essay responses to determine whether or not a potential student will be accepted to the university.\n",
    "\n",
    "The SAT has two sections of the test: Evidence-Based Reading and Writing and Math ([*source*](https://www.princetonreview.com/college/sat-sections)). The ACT has 4 sections: English, Mathematics, Reading, and Science, with an additional optional writing section ([*source*](https://www.act.org/content/act/en/products-and-services/the-act/scores/understanding-your-scores.html)). They have different score ranges, which you can read more about on their websites or additional outside sources (a quick Google search will help you understand the scores for each test):\n",
    "* [SAT](https://collegereadiness.collegeboard.org/sat)\n",
    "* [ACT](https://www.act.org/content/act/en.html)\n",
    "\n",
    "Standardized tests have long been a controversial topic for students, administrators, and legislators. Since the 1940's, an increasing number of colleges have been using scores from sudents' performances on tests like the SAT and the ACT as a measure for college readiness and aptitude ([*source*](https://www.minotdailynews.com/news/local-news/2017/04/a-brief-history-of-the-sat-and-act/)). Supporters of these tests argue that these scores can be used as an objective measure to determine college admittance. Opponents of these tests claim that these tests are not accurate measures of students potential or ability and serve as an inequitable barrier to entry. Lately, more and more schools are opting to drop the SAT/ACT requirement for their Fall 2021 applications ([*read more about this here*](https://www.cnn.com/2020/04/14/us/coronavirus-colleges-sat-act-test-trnd/index.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To-Do:** *Fill out this cell (or edit the above cell) with any other background or information that is necessary for your problem statement.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose your Data\n",
    "\n",
    "There are 10 datasets included in the [`data`](./data/) folder for this project. You are required to pick **at least two** of these to complete your analysis. Feel free to use more than two if you would like, or add other relevant datasets you find online.\n",
    "\n",
    "* [`act_2017.csv`](./data/act_2017.csv): 2017 ACT Scores by State\n",
    "* [`act_2018.csv`](./data/act_2018.csv): 2018 ACT Scores by State\n",
    "* [`act_2019.csv`](./data/act_2019.csv): 2019 ACT Scores by State\n",
    "* [`act_2019_ca.csv`](./data/act_2019_ca.csv): 2019 ACT Scores in California by School\n",
    "* [`sat_2017.csv`](./data/sat_2017.csv): 2017 SAT Scores by State\n",
    "* [`sat_2018.csv`](./data/sat_2018.csv): 2018 SAT Scores by State\n",
    "* [`sat_2019.csv`](./data/sat_2019.csv): 2019 SAT Scores by State\n",
    "* [`sat_2019_by_intended_college_major.csv`](./data/sat_2019_by_intended_college_major.csv): 2019 SAT Scores by Intended College Major\n",
    "* [`sat_2019_ca.csv`](./data/sat_2019_ca.csv): 2019 SAT Scores in California by School\n",
    "* [`sat_act_by_college.csv`](./data/sat_act_by_college.csv): Ranges of Accepted ACT & SAT Student Scores by Colleges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To-Do:** *Fill out this cell with the datasets you will use for your analysis. Write a brief description of the contents for each dataset that you choose.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TO COMPLETE\n",
    "#  (./data/sat_2019.csv): 2019 SAT Scores by State\n",
    "#* (./data/sat_2019_by_intended_college_major.csv):2019 SAT Scores by Intended College Major"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outside Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your problem statement and your chosen datasets, spend some time doing outside research on state policies or additional information that might be relevant. Summarize your findings below. If you bring in any outside tables or charts, make sure you are explicit about having borrowed them. If you quote any text, make sure that it renders as being quoted. **Make sure that you cite your sources.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To-Do:** *Fill out this cell with outside research or any additional background information that will support your analysis.*\n",
    " \n",
    "from 2022,PRO TEST https://www.theatlantic.com/ideas/archive/2022/04/mit-admissions-reinstates-sat-act-tests/629455/\n",
    "Inequalities in accessing college is due to lifelong social inequalities rather than  Stadardized testing:\n",
    "SAT / ACT actually seem to provide a levelling platform, as otherwise candidates would rely on extremely biaised means like essays or recommendation letters \n",
    "standardized testing is the lesser of two evils as MIT has chosen to re-instate it for fall 2023 admissions\n",
    "\n",
    "\n",
    "from 2019, AGAINST TEST https://www.washingtonpost.com/education/2019/03/19/is-it-finally-time-get-rid-sat-act-college-admissions-tests/\n",
    "seems to support that test - optional admissions actually yield good results. in TOP institutions,  graduation rates marginally differ between students who submitted a test upon their admission, and students who did not  \n",
    "also highlights a couple of scandals linked to tests\n",
    "\n",
    "Both articles argue that their view is helping students to get to high education. It reflects the deep inequalities that students face when applying for college."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Some states have a contract with the SAT to provide free tests to all juniors. States with existing SAT partnerships include Colorado, Connecticut, Delaware, District of Columbia, Idaho, Illinois, Indiana, Michigan, New Hampshire, Rhode Island, Texas, and West Virginia*<br>\n",
    "https://www.testgeek.com/blog/a-look-at-the-average-sat-score-by-state/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Problem statement could be*: <br>\n",
    "\"Does looking at SAT scores averages per state paints a correct picture of academic performance for a given state\"?\n",
    " \n",
    "FOCUS on SAT - \n",
    "https://sat.edu.sg/sat-guide/score-guide/#:~:text=Instead%2C%20the%20SAT%20is%20scored,might%20be%20harder%20than%20others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Challenges\n",
    "\n",
    "1. Manually calculate mean:\n",
    "\n",
    "    Write a function that takes in values and returns the mean of the values. Create a list of numbers that you test on your function to check to make sure your function works!\n",
    "    \n",
    "    *Note*: Do not use any mean methods built-in to any Python libraries to do this! This should be done without importing any additional libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_average (list_of_numbers): # creating function\n",
    "    my_sum = sum(list_of_numbers) # get the sum of list arguments\n",
    "    my_count = len(list_of_numbers) # get the count\n",
    "    final_average = my_sum / my_count # ratio for average\n",
    "    return final_average\n",
    "\n",
    " # try with random list\n",
    "my_list = [1,2,3,4,5,6,7,8,9,10,11]\n",
    "my_average (my_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Manually calculate standard deviation:\n",
    "\n",
    "    The formula for standard deviation is below:\n",
    "\n",
    "    $$\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu)^2}$$\n",
    "\n",
    "    Where $x_i$ represents each value in the dataset, $\\mu$ represents the mean of all values in the dataset and $n$ represents the number of values in the dataset.\n",
    "\n",
    "    Write a function that takes in values and returns the standard deviation of the values using the formula above. Hint: use the function you wrote above to calculate the mean! Use the list of numbers you created above to test on your function.\n",
    "    \n",
    "    *Note*: Do not use any standard deviation methods built-in to any Python libraries to do this! This should be done without importing any additional libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code:\n",
    "\n",
    "def standard_dev_func (x):\n",
    "    temp_list=[] #creating a storage list for each temp_value defined later\n",
    "    for each_val in x:\n",
    "        temp_val = (each_val - np.mean(x))**2 # focusing on the inside the square root part first\n",
    "        temp_list.append(temp_val) # append list ready for the sum\n",
    "    variance = sum(temp_list) / len(temp_list) # execute sum and get the average\n",
    "    stdev = round(variance **(1/2),2) # take the squareroot of x = x (exponent 0.5). rounding result at 2 decimals using the round function\n",
    "    return stdev\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\clement.galvier\\OneDrive - VIRGIN ACTIVE SINGAPORE PTE LTD\\Documents\\GeneralAssembly\\Projects\\project_1\\code\\starter-code_CG_08072022_1800.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/clement.galvier/OneDrive%20-%20VIRGIN%20ACTIVE%20SINGAPORE%20PTE%20LTD/Documents/GeneralAssembly/Projects/project_1/code/starter-code_CG_08072022_1800.ipynb#ch0000020?line=0'>1</a>\u001b[0m \u001b[39m#testing the function\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/clement.galvier/OneDrive%20-%20VIRGIN%20ACTIVE%20SINGAPORE%20PTE%20LTD/Documents/GeneralAssembly/Projects/project_1/code/starter-code_CG_08072022_1800.ipynb#ch0000020?line=1'>2</a>\u001b[0m my_new_list\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m4\u001b[39m,\u001b[39m5\u001b[39m,\u001b[39m6\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/clement.galvier/OneDrive%20-%20VIRGIN%20ACTIVE%20SINGAPORE%20PTE%20LTD/Documents/GeneralAssembly/Projects/project_1/code/starter-code_CG_08072022_1800.ipynb#ch0000020?line=2'>3</a>\u001b[0m standard_dev_func(my_new_list)\n",
      "\u001b[1;32mc:\\Users\\clement.galvier\\OneDrive - VIRGIN ACTIVE SINGAPORE PTE LTD\\Documents\\GeneralAssembly\\Projects\\project_1\\code\\starter-code_CG_08072022_1800.ipynb Cell 21\u001b[0m in \u001b[0;36mstandard_dev_func\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/clement.galvier/OneDrive%20-%20VIRGIN%20ACTIVE%20SINGAPORE%20PTE%20LTD/Documents/GeneralAssembly/Projects/project_1/code/starter-code_CG_08072022_1800.ipynb#ch0000020?line=3'>4</a>\u001b[0m temp_list\u001b[39m=\u001b[39m[] \u001b[39m#creating a storage list for each temp_value defined later\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/clement.galvier/OneDrive%20-%20VIRGIN%20ACTIVE%20SINGAPORE%20PTE%20LTD/Documents/GeneralAssembly/Projects/project_1/code/starter-code_CG_08072022_1800.ipynb#ch0000020?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m each_val \u001b[39min\u001b[39;00m x:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/clement.galvier/OneDrive%20-%20VIRGIN%20ACTIVE%20SINGAPORE%20PTE%20LTD/Documents/GeneralAssembly/Projects/project_1/code/starter-code_CG_08072022_1800.ipynb#ch0000020?line=5'>6</a>\u001b[0m     temp_val \u001b[39m=\u001b[39m (each_val \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mmean(x))\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m# focusing on the inside the square root part first\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/clement.galvier/OneDrive%20-%20VIRGIN%20ACTIVE%20SINGAPORE%20PTE%20LTD/Documents/GeneralAssembly/Projects/project_1/code/starter-code_CG_08072022_1800.ipynb#ch0000020?line=6'>7</a>\u001b[0m     temp_list\u001b[39m.\u001b[39mappend(temp_val) \u001b[39m# append list ready for the sum\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/clement.galvier/OneDrive%20-%20VIRGIN%20ACTIVE%20SINGAPORE%20PTE%20LTD/Documents/GeneralAssembly/Projects/project_1/code/starter-code_CG_08072022_1800.ipynb#ch0000020?line=7'>8</a>\u001b[0m variance \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(temp_list) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(temp_list) \u001b[39m# execute sum and get the average\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#testing the function\n",
    "my_new_list=[1,2,3,4,5,6]\n",
    "standard_dev_func(my_new_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Data cleaning function:\n",
    "    \n",
    "    Write a function that takes in a string that is a number and a percent symbol (ex. '50%', '30.5%', etc.) and converts this to a float that is the decimal approximation of the percent. For example, inputting '50%' in your function should return 0.5, '30.5%' should return 0.305, etc. Make sure to test your function to make sure it works!\n",
    "\n",
    "You will use these functions later on in the project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the function\n",
    "def str_to_float (some_numbers):\n",
    "    try:\n",
    "        if \"%\" in some_numbers:\n",
    "            string_no_percent= some_numbers.replace('%','') # using replace method\n",
    "            extracted_value = round(float(string_no_percent),2) #converting into a rounded number\n",
    "            decimal = extracted_value/100\n",
    "            return decimal\n",
    "            \n",
    "        if some_numbers.isempty() == True or some_numbers == np.nan:\n",
    "            return 0.0\n",
    "        else :\n",
    "            return 0.0 #for the sat_2019 case where - are converted into \"Nan\"\n",
    "    except:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the function\n",
    "str_to_float('48.887%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# Part 2\n",
    "\n",
    "Part 2 requires knowledge of Pandas, EDA, data cleaning, and data visualization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*All libraries used should be added here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import numpy as np # for numerical and stats functionalities\n",
    "import pandas as pd # for working with tables\n",
    "import matplotlib as plt # for dataviz\n",
    "import seaborn as sns # for dataviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import & Cleaning\n",
    "\n",
    "Import the datasets that you selected for this project and go through the following steps at a minimum. You are welcome to do further cleaning as you feel necessary:\n",
    "1. Display the data: print the first 5 rows of each dataframe to your Jupyter notebook.\n",
    "2. Check for missing values.\n",
    "3. Check for any obvious issues with the observations (keep in mind the minimum & maximum possible values for each test/subtest).\n",
    "4. Fix any errors you identified in steps 2-3.\n",
    "5. Display the data types of each feature.\n",
    "6. Fix any incorrect data types found in step 5.\n",
    "    - Fix any individual values preventing other columns from being the appropriate type.\n",
    "    - If your dataset has a column of percents (ex. '50%', '30.5%', etc.), use the function you wrote in Part 1 (coding challenges, number 3) to convert this to floats! *Hint*: use `.map()` or `.apply()`.\n",
    "7. Rename Columns.\n",
    "    - Column names should be all lowercase.\n",
    "    - Column names should not contain spaces (underscores will suffice--this allows for using the `df.column_name` method to access columns in addition to `df['column_name']`).\n",
    "    - Column names should be unique and informative.\n",
    "8. Drop unnecessary rows (if needed).\n",
    "9. Merge dataframes that can be merged.\n",
    "10. Perform any additional cleaning that you feel is necessary.\n",
    "11. Save your cleaned and merged dataframes as csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1. Display the data: print the first 5 rows of each dataframe to your Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1 loading the files - focusing on SAT trends\n",
    "sat_2017_df = pd.read_csv('../data/sat_2017.csv')\n",
    "sat_2018_df = pd.read_csv('../data/sat_2018.csv')\n",
    "sat_2019_df = pd.read_csv('../data/sat_2019.csv') \n",
    "sat_2019_intended_major = pd.read_csv('../data/sat_2019_by_intended_college_major.csv') #:2019 SAT Scores by Intended College Major, \n",
    "\n",
    "# #1.2 displayin the first five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2017_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2018_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2019_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*the 3 years of SAT display the same info with different column names*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2019_intended_major.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2 - check for missing values (we could also use .info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(sat_2017_df.isna().sum()) \n",
    "print(sat_2018_df.isna().sum())\n",
    "print(sat_2019_df.isna().sum())\n",
    "print(sat_2019_intended_major.isna().sum())\n",
    "# NO MISSING VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3. Check for any obvious issues with the observations (keep in mind the minimum & maximum possible values for each test/subtest)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2018_df.info() # participation should be a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2017_df.describe(include='all') # 2017 dataset : math and EBRW min shd be min 200 and max 800. however sat_2017 has 1 outlier on math and score total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2018_df.describe(include='all') #all metrics are reasonable, within range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2018_df.info() # participation should be a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2018_df['Participation'].unique() # Column transformation for Participation field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2019_df.describe(include='all') # jumped to 53 states | #all metrics are reasonable, within range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2019_df.info() # participation should be a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2019_df['Participation Rate'].unique() # We have a problem here with a \"-\" in a numerical field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at by college major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2019_intended_major.describe(include='all') # all metrics are reasonable, within range. 9 unique percent most likely due to 0% rounded, that we find multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2019_intended_major.info() # percent should be a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2019_intended_major['Percent'].unique() # Ccolumn transformation is waiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4. Fix any errors you identified in steps 2-3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets start with 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets focus on sat_2017_df.describe\n",
    "sat_2017_df['Math'].sort_values(ascending=True).head(5) # checking where is the outlier, index 20. \n",
    "#we can see there is only 1 outlier as the values are sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sat_2017_df.loc[sat_2017_df['Math'] == 52, :].head()) # we can derive the actual value by taking total - EBRW and replace it inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2017_df['Math'][20] # checking that we are targeting the correct value to replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2017_df.loc[20,['Math']] = 524 # replacing with .loc function, 524 = 1060(total_score) - 536 (EBWR score) = math score = 524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2017_df.loc[20] # checking absolute result and in table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue with 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's replace \"-\" in sat_2019_df by Nan values\n",
    "print(sat_2019_df.loc[sat_2019_df['Participation Rate'] == \"—\" , :].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2019_df['Participation Rate'] = sat_2019_df['Participation Rate'].replace('—','0%') # trying out a different technique to replace values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2019_df['Participation Rate'].unique() # testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5. Display the data types of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2017_df.info() # participation shd be %  instead of object, we will fix this in step 6\n",
    "sat_2018_df.info() # participation shd be %  instead of object, we will fix this in step 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2019_df.info() # participation shd be % instead of object - we will fix this in step6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2019_intended_major.info() # percent shd be % instead of object - we will fix in step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#6. Fix any incorrect data types found in step 5.\n",
    "    #Fix any individual values preventing other columns from being the appropriate type.\n",
    "    # If your dataset has a column of percents (ex. '50%', '30.5%', etc.), use the function you wrote in Part 1 (coding challenges, number 3) to convert this to floats! \n",
    "    # *Hint*: use `.map()` or `.apply()`.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2017_df[\"part_per\"] = sat_2017_df['Participation'].apply(str_to_float) # create a new column to store tranformed values\n",
    "sat_2018_df[\"part_per\"] = sat_2018_df['Participation'].apply(str_to_float) # create a new column to store tranformed values\n",
    "sat_2019_df[\"part_per\"] = sat_2019_df['Participation Rate'].apply(str_to_float) # create a new column to store tranformed values\n",
    "sat_2019_intended_major['part_per']=sat_2019_intended_major['Percent'].apply(str_to_float) # create a new column to store tranformed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2019_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2019_intended_major.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#7. Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     - Column names should be all lowercase.\n",
    "#     - Column names should not contain spaces (underscores will suffice--this allows for using the `df.column_name` method to access columns in addition to `df['column_name']`).\n",
    "#     - Column names should be unique and informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same technique as            ufo.columns = ufo.columns.str.lower() but since we need to add the \"_\", I rename manually\n",
    "sat_2017_df.rename(columns={\n",
    "    'State': 'state',\n",
    "    'Participation': 'participation',\n",
    "    'Evidence-Based Reading and Writing': 'score_read_write',\n",
    "    'Math':'score_math',\n",
    "    'Total':'score_total',\n",
    "    \"part_per\":'participation_percent'\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "sat_2018_df.rename(columns={\n",
    "    'State': 'state',\n",
    "    'Participation': 'participation',\n",
    "    'Evidence-Based Reading and Writing': 'score_read_write',\n",
    "    'Math':'score_math',\n",
    "    'Total':'score_total',\n",
    "    \"part_per\":'participation_percent'\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "sat_2019_df.rename(columns={\n",
    "    'State': 'state',\n",
    "    'Participation Rate': 'participation',\n",
    "    'EBRW': 'score_read_write',\n",
    "    'Math':'score_math',\n",
    "    'Total':'score_total',\n",
    "    \"part_per\":'participation_percent'\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "sat_2019_intended_major.rename(columns={\n",
    "    'IntendedCollegeMajor': 'intended_major',\n",
    "    'TestTakers': 'num_of_participants',\n",
    "    'Percent': 'percent',\n",
    "    'Total':'score_total',\n",
    "    'ReadingWriting':'score_read_write',\n",
    "    \"Math\":'score_math',\n",
    "    'part_per':'participation_percent'\n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#8. Drop unnecessary rows (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#9. Merge dataframes that can be merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before mering 3 years of SAT resutlts, we must add a differentiating column called \"year\" in each of them\n",
    "sat_2017_df['year'] =2017\n",
    "sat_2018_df['year'] =2018\n",
    "sat_2019_df['year'] = 2019\n",
    "\n",
    "#leaving the college major one on the side for now\n",
    "sat_2019_intended_major['year'] =2019 # let's add a year name to this df just in case\n",
    "\n",
    "#then we can concat \n",
    "sat_three_years= pd.concat([sat_2017_df, sat_2018_df,sat_2019_df]) # only merging similar df\n",
    "sat_three_years.shape\n",
    "sat_three_years.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#10. Perform any additional cleaning that you feel is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#>> lets drop the old participation object\n",
    "sat_three_years.drop(columns=['participation'], axis=1,inplace= True)\n",
    "sat_2019_intended_major.drop(columns=['percent'],axis=1,inplace=True)\n",
    "sat_2019_intended_major.drop(columns=['num_of_participants'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_three_years.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2019_intended_major.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#11 Save your cleaned and merged dataframes as csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('output'):\n",
    "    os.makedirs('output')\n",
    "sat_three_years.to_csv('output/sat_three_years.csv')\n",
    "sat_2019_intended_major.to_csv('output/sat_intended_major.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "\n",
    "Now that we've fixed our data, and given it appropriate names, let's create a [data dictionary](http://library.ucmerced.edu/node/10249). \n",
    "\n",
    "A data dictionary provides a quick overview of features/variables/columns, alongside data types and descriptions. The more descriptive you can be, the more useful this document is.\n",
    "\n",
    "Example of a Fictional Data Dictionary Entry: \n",
    "\n",
    "|Feature|Type|Dataset|Description|\n",
    "|---|---|---|---|\n",
    "|**county_pop**|*integer*|2010 census|The population of the county (units in thousands, where 2.5 represents 2500 people).| \n",
    "|**per_poverty**|*float*|2010 census|The percent of the county over the age of 18 living below the 200% of official US poverty rate (units percent to two decimal places 98.10 means 98.1%)|\n",
    "\n",
    "[Here's a quick link to a short guide for formatting markdown in Jupyter notebooks](https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html).\n",
    "\n",
    "Provided is the skeleton for formatting a markdown table, with columns headers that will help you create a data dictionary to quickly summarize your data, as well as some examples. **This would be a great thing to copy and paste into your custom README for this project.**\n",
    "\n",
    "*Note*: if you are unsure of what a feature is, check the source of the data! This can be found in the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_three_years.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To-Do:** *Edit the table below to create your own data dictionary for the datasets you chose.*\n",
    "\n",
    "|Feature|Type|Dataset|Description|\n",
    "|---|---|---|---|\n",
    "|**state**|object|sat_three_years|states where SAT was taken|\n",
    "|**score_read_write**|int|sat_three_years|score obtained in reading and writing|\n",
    "|**score_math**|int|sat_three_years|score obtained in math|\n",
    "|**score_total**|int|sat_three_years|sum of reading_writing and math|\n",
    "|**participation_percent**|float|sat_three_years|% of the student population who have taken the test|\n",
    "|**year**|int|sat_three_years|year of the test|\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Feature|Type|Dataset|Description|\n",
    "|---|---|---|---|\n",
    "|**intended_major**|object|sat_2019_intended_major|Major intended to be taken if pass test with required |\n",
    "|**score_total**|int|sat_2019_intended_major|sum of reading_writing and math|\n",
    "|**score_math**|int|sat_2019_intended_major|score obtained in math|\n",
    "|**score_read_write**|int|sat_2019_intended_major|score obtained on in reading and writing|\n",
    "|**participation_percent**|float|sat_2019_intended_major|% of the student population who have taken the test|\n",
    "|**year**|int|sat_2019_intended_major|year of the test|\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Complete the following steps to explore your data. You are welcome to do more EDA than the steps outlined here as you feel necessary:\n",
    "1. Summary Statistics.\n",
    "2. Use a **dictionary comprehension** to apply the standard deviation function you create in part 1 to each numeric column in the dataframe.  **No loops**.\n",
    "    - Assign the output to variable `sd` as a dictionary where: \n",
    "        - Each column name is now a key \n",
    "        - That standard deviation of the column is the value \n",
    "        - *Example Output :* `{'ACT_Math': 120, 'ACT_Reading': 120, ...}`\n",
    "3. Investigate trends in the data.\n",
    "    - Using sorting and/or masking (along with the `.head()` method to avoid printing our entire dataframe), consider questions relevant to your problem statement. Some examples are provided below (but feel free to change these questions for your specific problem):\n",
    "        - Which states have the highest and lowest participation rates for the 2017, 2019, or 2019 SAT and ACT?\n",
    "        - Which states have the highest and lowest mean total/composite scores for the 2017, 2019, or 2019 SAT and ACT?\n",
    "        - Do any states with 100% participation on a given test have a rate change year-to-year?\n",
    "        - Do any states show have >50% participation on *both* tests each year?\n",
    "        - Which colleges have the highest median SAT and ACT scores for admittance?\n",
    "        - Which California school districts have the highest and lowest mean test scores?\n",
    "    - **You should comment on your findings at each step in a markdown cell below your code block**. Make sure you include at least one example of sorting your dataframe by a column, and one example of using boolean filtering (i.e., masking) to select a subset of the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do a summary of main statistics for all the 2 df ( sat_three_years and sat_2019_intended_major)\n",
    "*mean*<br>\n",
    "*std*<br>\n",
    "*min and max*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#lets look at the mean for our concatenated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #Code:\n",
    "sat_three_years.groupby(by='year')['score_total','score_math','score_read_write'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#lets look at the std for our concatenated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_three_years.groupby(by='year')['score_total','score_math','score_read_write'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#lets look at the min for our concatenated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_three_years.groupby(by='year')['score_total','score_math','score_read_write'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#lets look at the max for our concatenated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_three_years.groupby(by='year')['score_total','score_math','score_read_write'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#lets look at all 3 metrics for the intended major file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2019_intended_major.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use a **dictionary comprehension** to apply the standard deviation function you create in part 1 to each numeric column in the dataframe.  **No loops**.\n",
    "    - Assign the output to variable `sd` as a dictionary where: \n",
    "        - Each column name is now a key \n",
    "        - That standard deviation of the column is the value \n",
    "        - *Example Output :* `{'ACT_Math': 120, 'ACT_Reading': 120, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We would like our final output to look like this:\n",
    "final_dict = {\n",
    "    2017:\n",
    "    {'score_math':20,\n",
    "    'score_ebrw':20,\n",
    "    'score_total':30}\n",
    "    ,\n",
    "    2018:\n",
    "    {'score_math':20,\n",
    "    'score_ebrw':20,\n",
    "    'score_total':30}\n",
    "    ,\n",
    "    2019:\n",
    "    {'score_math':20,\n",
    "    'score_ebrw':20,\n",
    "    'score_total':30}\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#we will only do it for the concatenated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years =(sat_three_years[\"year\"]).unique() # to iterate over the years in the concatenated file\n",
    "\n",
    "sd = {year:\n",
    "        {\n",
    "        column:round(np.std(sat_three_years[column]),3) # inner part of the loop \n",
    "        for column in list(sat_three_years.columns[1:4])# inner part of the loop\n",
    "        }\n",
    "        for year in list(years)}# outer part of loop\n",
    "sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Investigate trends in the data.\n",
    "    - Using sorting and/or masking (along with the `.head()` method to avoid printing our entire dataframe), consider questions relevant to your problem statement. Some examples are provided below (but feel free to change these questions for your specific problem):\n",
    "        - Which states have the highest and lowest participation rates for the 2017, 2019, or 2019 SAT and ACT?\n",
    "        - Which states have the highest and lowest mean total/composite scores for the 2017, 2019, or 2019 SAT and ACT?\n",
    "        - Do any states with 100% participation on a given test have a rate change year-to-year?\n",
    "        - Do any states show have >50% participation on *both* tests each year?\n",
    "        - Which colleges have the highest median SAT and ACT scores for admittance?\n",
    "        - Which California school districts have the highest and lowest mean test scores?\n",
    "    - **You should comment on your findings at each step in a markdown cell below your code block**. Make sure you include at least one example of sorting your dataframe by a column, and one example of using boolean filtering (i.e., masking) to select a subset of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#which state have the highest participation rate per year\n",
    "print(sat_three_years.loc[sat_three_years[\"year\"]==2017,['participation_percent','state']].sort_values(\"participation_percent\",ascending=False).head(7))\n",
    "print(sat_three_years.loc[sat_three_years[\"year\"]==2018,['participation_percent','state']].sort_values(\"participation_percent\",ascending=False).head(10))\n",
    "print(sat_three_years.loc[sat_three_years[\"year\"]==2019,['participation_percent','state']].sort_values(\"participation_percent\",ascending=False).head(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top participation score states are ex-aequo at 100% : Connecticut, Colorado, DC, Columbia and Delaware, year over year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Most of the states maintain their participation rate at the same level\n",
    " ##### it seems to be mandatory to take SAT for these states <br>\n",
    " https://prepmaven.com/blog/test-prep/states-require-sat-act/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Do any states with 100% participation on a given test have a rate change year-to-year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#which state have the lowest participation rate per year\n",
    "print(sat_three_years.loc[sat_three_years[\"year\"]==2017,['participation_percent','state']].sort_values(\"participation_percent\",ascending=True).head(3))\n",
    "print(sat_three_years.loc[sat_three_years[\"year\"]==2018,['participation_percent','state']].sort_values(\"participation_percent\",ascending=True).head(3))\n",
    "print(sat_three_years.loc[sat_three_years[\"year\"]==2019,['participation_percent','state']].sort_values(\"participation_percent\",ascending=True).head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowest participation score states were Virgin Islands, Puerto Rico and North Dakota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking whether we have any obvious correlations for a given year in the dataset\n",
    "sat_three_years.loc[sat_three_years['year']==2017].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**  \n",
    "*Read_write and math scores are highly correlated (which makes sense : it would be surprising for a state to be significantly worse at math compared to read_write for example )*\n",
    "<br>\n",
    "\n",
    "*however participation % is highly negatively correlated with score_total, which is surprising. Checking 2018 and 2019 to confirm whether it's a trend:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking whether we have any obvious correlations for a given year in the dataset\n",
    "sat_three_years.loc[sat_three_years['year']==2018].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking whether we have any obvious correlations for a given year in the dataset\n",
    "sat_three_years.loc[sat_three_years['year']==2019].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** *correlation is confirmed between participation and score_total. however, correlation is not causation: we cannot yet deduct from here that because the % rate is low, then the score total is high.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Data\n",
    "\n",
    "There's not a magic bullet recommendation for the right number of plots to understand a given dataset, but visualizing your data is *always* a good idea. Not only does it allow you to quickly convey your findings (even if you have a non-technical audience), it will often reveal trends in your data that escaped you when you were looking only at numbers. It is important to not only create visualizations, but to **interpret your visualizations** as well.\n",
    "\n",
    "**Every plot should**:\n",
    "- Have a title\n",
    "- Have axis labels\n",
    "- Have appropriate tick labels\n",
    "- Text is legible in a plot\n",
    "- Plots demonstrate meaningful and valid relationships\n",
    "- Have an interpretation to aid understanding\n",
    "\n",
    "Here is an example of what your plots should look like following the above guidelines. Note that while the content of this example is unrelated, the principles of visualization hold:\n",
    "\n",
    "![](https://snag.gy/hCBR1U.jpg)\n",
    "*Interpretation: The above image shows that as we increase our spending on advertising, our sales numbers also tend to increase. There is a positive correlation between advertising spending and sales.*\n",
    "\n",
    "---\n",
    "\n",
    "Here are some prompts to get you started with visualizations. Feel free to add additional visualizations as you see fit:\n",
    "1. Use Seaborn's heatmap with pandas `.corr()` to visualize correlations between all numeric features.\n",
    "    - Heatmaps are generally not appropriate for presentations, and should often be excluded from reports as they can be visually overwhelming. **However**, they can be extremely useful in identify relationships of potential interest (as well as identifying potential collinearity before modeling).\n",
    "    - Please take time to format your output, adding a title. Look through some of the additional arguments and options. (Axis labels aren't really necessary, as long as the title is informative).\n",
    "2. Visualize distributions using histograms. If you have a lot, consider writing a custom function and use subplots.\n",
    "    - *OPTIONAL*: Summarize the underlying distributions of your features (in words & statistics)\n",
    "         - Be thorough in your verbal description of these distributions.\n",
    "         - Be sure to back up these summaries with statistics.\n",
    "         - We generally assume that data we sample from a population will be normally distributed. Do we observe this trend? Explain your answers for each distribution and how you think this will affect estimates made from these data.\n",
    "3. Plot and interpret boxplots. \n",
    "    - Boxplots demonstrate central tendency and spread in variables. In a certain sense, these are somewhat redundant with histograms, but you may be better able to identify clear outliers or differences in IQR, etc.\n",
    "    - Multiple values can be plotted to a single boxplot as long as they are of the same relative scale (meaning they have similar min/max values).\n",
    "    - Each boxplot should:\n",
    "        - Only include variables of a similar scale\n",
    "        - Have clear labels for each variable\n",
    "        - Have appropriate titles and labels\n",
    "4. Plot and interpret scatter plots to view relationships between features. Feel free to write a custom function, and subplot if you'd like. Functions save both time and space.\n",
    "    - Your plots should have:\n",
    "        - Two clearly labeled axes\n",
    "        - A proper title\n",
    "        - Colors and symbols that are clear and unmistakable\n",
    "5. Additional plots of your choosing.\n",
    "    - Are there any additional trends or relationships you haven't explored? Was there something interesting you saw that you'd like to dive further into? It's likely that there are a few more plots you might want to generate to support your narrative and recommendations that you are building toward. **As always, make sure you're interpreting your plots as you go**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use Seaborn's heatmap with pandas `.corr()` to visualize correlations between all numeric features.\n",
    "    - Heatmaps are generally not appropriate for presentations, and should often be excluded from reports as they can be visually overwhelming. **However**, they can be extremely useful in identify relationships of potential interest (as well as identifying potential collinearity before modeling).\n",
    "    - Please take time to format your output, adding a title. Look through some of the additional arguments and options. (Axis labels aren't really necessary, as long as the title is informative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the correlation matrix and heatmap for SAT 2019 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2019_df_corr =sat_three_years.loc[sat_three_years['year']==2019].corr() # filtering on 1 year of the data\n",
    "mask = np.zeros_like(sat_2019_df_corr)\n",
    "\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    s = sns.heatmap(sat_2019_df_corr,mask=mask,square= True, annot = True,  vmin = -1, vmax = 1, linewidths = .5)\n",
    "    s.set(xlabel='Explained Variable', ylabel='', title= 'Correlation matrix, year 2019')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same conclusion arises with negative correlation between score_total and participation_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Visualize distributions using histograms. If you have a lot, consider writing a custom function and use subplots.\n",
    "    - *OPTIONAL*: Summarize the underlying distributions of your features (in words & statistics)\n",
    "         - Be thorough in your verbal description of these distributions.\n",
    "         - Be sure to back up these summaries with statistics.\n",
    "         - We generally assume that data we sample from a population will be normally distributed. Do we observe this trend? Explain your answers for each distribution and how you think this will affect estimates made from these data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### plotting distribution of scores for 2017,2018,2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2017 = sns.histplot(sat_three_years.loc[sat_three_years['year']==2017,['score_total']], x=\"score_total\", bins = 10)\n",
    "plot_2017.set(title=\"2017 SAT scores disctribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2018 = sns.histplot(sat_three_years.loc[sat_three_years['year']==2018,['score_total']], x=\"score_total\", bins = 10)\n",
    "plot_2018.set(title=\"2018 SAT scores disctribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2019 = sns.histplot(sat_three_years.loc[sat_three_years['year']==2019,['score_total']], x=\"score_total\",bins = 10)\n",
    "plot_2019.set(title=\"2019 SAT scores disctribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### None of the 3 years score_total is normally distributed - we see that results mostly range between [800- 1100] or [1200 and above]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Plot and interpret boxplots. \n",
    "    - Boxplots demonstrate central tendency and spread in variables. In a certain sense, these are somewhat redundant with histograms, but you may be better able to identify clear outliers or differences in IQR, etc.\n",
    "    - Multiple values can be plotted to a single boxplot as long as they are of the same relative scale (meaning they have similar min/max values).\n",
    "    - Each boxplot should:\n",
    "        - Only include variables of a similar scale\n",
    "        - Have clear labels for each variable\n",
    "        - Have appropriate titles and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#lets look at the 3 years side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "boxplot_years = ax = sns.boxplot(x=sat_three_years['year'] , y=sat_three_years['score_total'])\n",
    "boxplot_years.set_title(\"score total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#breaking the years by subject: math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_math = sns.boxplot(x=sat_three_years['year'] , y=sat_three_years['score_math'])\n",
    "boxplot_math.set_title(\"score math\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_ebrw = sns.boxplot(x=sat_three_years['year'] , y=sat_three_years['score_read_write'])\n",
    "boxplot_ebrw.set_title(\"score reading_writing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we visually see the wider sd in 2019, confirming the table line [std](#std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Plot and interpret scatter plots to view relationships between features. Feel free to write a custom function, and subplot if you'd like. Functions save both time and space.\n",
    "    - Your plots should have:\n",
    "        - Two clearly labeled axes\n",
    "        - A proper title\n",
    "        - Colors and symbols that are clear and unmistakable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create scatterplot participation_percent vs score_total for year 2019.\n",
    "plt.scatter(sat_three_years.loc[sat_three_years['year']==2019, ['score_total']],\n",
    "            sat_three_years.loc[sat_three_years['year']==2019, ['participation_percent']]);\n",
    "plt.title(\"SAT - relation between score_total and participation_percent, year 2019 \", fontsize =12)\n",
    "plt.ylabel(\"Participation_percent (x100)\", fontsize = 10)\n",
    "plt.xlabel(\"score_total\", fontsize = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*correlation confirmed by this chart for the year 2019*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#looking at the chart by intended major "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create scatterplot participation_percent vs score_total for year 2019.\n",
    "plt.scatter(sat_2019_intended_major['score_total'],\n",
    "            sat_2019_intended_major['participation_percent']);\n",
    "plt.title(\"SAT - relation between score_total and participation_percent, year 2019\")\n",
    "plt.xlabel(\"score_total\", fontsize = 10)\n",
    "plt.ylabel(\"participation_percent by intended major\", fontsize = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#we can see any correlation between participation_percent and score total, because the participants for 1 given major are scattered across SAT-mandatory and non-mandatory states, hence blurring the picture\n",
    "It would be helpful to have the same table with an extra \"state\" dimension to cross-reference participation rate, state and score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your exploration of the data, what are you key takeaways and recommendations? Make sure to answer your question of interest or address your problem statement here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To-Do:** <br>\n",
    "*SAT Score results seem heavily correlated to the % of participation in each state. this is verified for the last 3 years* <br>\n",
    "*This first element points to a lot of determinism in these tests*<br>\n",
    "*Furthermore, the top participation states remain the same year over year, reducing the opportunities for students in high participation levels - which also points to determinism*<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to create your README!\n",
    "\n",
    "**To-Do:** *If you combine your problem statement, data dictionary, brief summary of your analysis, and conclusions/recommendations, you have an amazing README.md file that quickly aligns your audience to the contents of your project.* Don't forget to cite your data sources!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f71697013d76e192046347603e9541ea88dc58adcc0e0b5a71eda0b6e40070d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
